---
title: "2Assignment5"
---

### First 10 documents download
```{r, warning=FALSE, message=FALSE}
## Scraping Government data
## Website: GovInfo (https://www.govinfo.gov/app/search/)
## Prerequisite: Download from website the list of files to be downloaded
## Designed for background job

# Start with a clean plate and lean loading to save memory
#gc(reset=T)
# install.packages(c("purrr", "magrittr")
library(purrr)
library(magrittr) # Alternatively, load tidyverse

##setwd("___")
# install.packages("rjson")
library(rjson)
library(jsonlite)
library(data.table)
library(readr)

## CSV method
govfiles= read.csv(file="https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_10_42.csv", skip=2)

## JSON method
### rjson
gf_list <- rjson::fromJSON(file ="https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json")
govfile2=dplyr::bind_rows(gf_list$resultSet)

### jsonlite
gf_list1 = jsonlite::read_json("https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json")
govfiles3 <- gf_list1$resultSet
govfiles3 <- gf_list1$resultSet |> dplyr::bind_rows()

# Preparing for bulk download of government documents
govfiles$id = govfiles$packageId
pdf_govfiles_url = govfiles3$pdfLink
pdf_govfiles_id <- govfiles3$index

# Directory to save the pdf's
# Be sure to create a folder for storing the pdf's
save_dir <- "C:/Users/chant/OneDrive/Documents/Chantan NEW Quarto Website/assets/govfiles/"

# Function to download pdfs
download_govfiles_pdf <- function(url, id) {
  tryCatch({
    destfile <- paste0(save_dir, "govfiles_", id, ".pdf")
    download.file(url, destfile = destfile, mode = "wb") # Binary files
    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of "hacking" the server
    return(paste("Successfully downloaded:", url))
  },
  error = function(e) {
    return(paste("Failed to download:", url))
  })
}

# Downloading first 10 files
start.time <- Sys.time()
message("Starting downloads")
results <- 1:10 %>% 
  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))
message("Finished downloads")
end.time <- Sys.time()
time.taken <- end.time - start.time

print(results)
```
### Congressional Hearings example
```{r, warning=FALSE, message=FALSE}
## Exercise: Try downloading 118th Congress Congressional Hearings in Committee on Foreign Affairs?
govfiles_ex <- govfiles3 %>% 
  dplyr::filter(collection == "Congressional Hearings")

govfiles_ex$id = govfiles_ex$packageId
pdf_govfilesex_url = govfiles_ex$pdfLink
pdf_govfilesex_id <- govfiles_ex$index

save_dir <- "C:/Users/chant/OneDrive/Documents/Chantan NEW Quarto Website/assets/govfiles/"

download_govfilesex_pdf <- function(pdf_govfilesex_url,id) {
  tryCatch({
    destfile <- paste0(save_dir, "govfiles_ex_", id, ".pdf")
    download.file(pdf_govfilesex_url, destfile = destfile, mode = "wb")
    Sys.sleep(runif(1, 1, 3)) 
    return(paste("Successfully downloaded:", pdf_govfilesex_url))
  },
  error = function(e) {
    return(paste("Failed to download:", pdf_govfilesex_url))
  })
}

start.time <- Sys.time()
message("Starting downloads")
results_ex <- 1:length(pdf_govfilesex_url) %>% 
  purrr::map_chr(~ download_govfilesex_pdf(pdf_govfilesex_url[.], pdf_govfilesex_id[.]))
message("Finished downloads")
end.time <- Sys.time()
time.taken <- end.time - start.time

print(results_ex)
```

### Report
The main difficulty in running the code for the scrapping process is understanding exactly what was happening. However, after really looking at the code and testing certain changes, I was able to edit the original code to download what I was specifically asking for to complete the Exercise.
The data scrapped that makes up the dataframes (i.e. the govfile#) are very usable and easy to navigate. I was able to filter the dataframe "govfile3" in order to download the specific documents I needed with ease. The only improvements I could think of was getting rid of the unnecessary variables that only complicate the process of looking for the correct information. This includes the "teaser" and "otherLink" variables.





